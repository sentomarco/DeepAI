{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZdFEd2Y4VDs"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x  # Colab only.\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "# More imports\n",
        "from tensorflow.keras.layers import Input, SimpleRNN, GRU, LSTM, Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Load in the data\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "print(\"x_train.shape:\", x_train.shape)\n",
        "\n",
        "# Build the model\n",
        "i = Input(shape=x_train[0].shape)\n",
        "x = LSTM(128)(i)\n",
        "x = Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(i, x)\n",
        "\n",
        "\n",
        "# Compile and train\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "r = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot loss per iteration\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(r.history['loss'], label='loss')\n",
        "plt.plot(r.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "Xuma9p8J4iIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot loss per iteration\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(r.history['loss'], label='loss')\n",
        "plt.plot(r.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "vJq3hlW34juJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si raggiunge una accuratezza elevata dopo appena 10 epoche, è questo è un long term classification perchè le immagini sono di lunghezza 28 (prima con appena 30 andava in crisi LSTM 28 ce la fa ancora bene) ed è un long term classification problem perchè non bastano gli utimi pixel per classificare (x es gli ultimi pixel sono tutti neri) "
      ],
      "metadata": {
        "id": "MH-DOTDb5Kyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "  \"\"\"\n",
        "  This function prints and plots the confusion matrix.\n",
        "  Normalization can be applied by setting `normalize=True`.\n",
        "  \"\"\"\n",
        "  if normalize:\n",
        "      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "      print(\"Normalized confusion matrix\")\n",
        "  else:\n",
        "      print('Confusion matrix, without normalization')\n",
        "\n",
        "  print(cm)\n",
        "\n",
        "  plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "  plt.title(title)\n",
        "  plt.colorbar()\n",
        "  tick_marks = np.arange(len(classes))\n",
        "  plt.xticks(tick_marks, classes, rotation=45)\n",
        "  plt.yticks(tick_marks, classes)\n",
        "\n",
        "  fmt = '.2f' if normalize else 'd'\n",
        "  thresh = cm.max() / 2.\n",
        "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "      plt.text(j, i, format(cm[i, j], fmt),\n",
        "               horizontalalignment=\"center\",\n",
        "               color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "p_test = model.predict(x_test).argmax(axis=1)\n",
        "cm = confusion_matrix(y_test, p_test)\n",
        "plot_confusion_matrix(cm, list(range(10)))\n",
        "\n",
        "# Do these results make sense?\n",
        "# It's easy to confuse 9 <--> 4, 9 <--> 7, 2 <--> 7, etc. \n"
      ],
      "metadata": {
        "id": "Bcom6lRo4lQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show some misclassified examples\n",
        "misclassified_idx = np.where(p_test != y_test)[0]\n",
        "i = np.random.choice(misclassified_idx)\n",
        "plt.imshow(x_test[i], cmap='gray')\n",
        "plt.title(\"True label: %s Predicted: %s\" % (y_test[i], p_test[i]));"
      ],
      "metadata": {
        "id": "B-eMv45h4m30"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}