{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhNW0aYQnx2t"
      },
      "outputs": [],
      "source": [
        "#Exercise to prove that LSTM are better in tracking long term dependecies\n",
        "\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x  # Colab only.\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "# More imports\n",
        "from tensorflow.keras.layers import Input, SimpleRNN, GRU, LSTM, Dense, Flatten, GlobalMaxPool1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "### build the dataset\n",
        "# This is a nonlinear AND long-distance dataset\n",
        "# (Actually, we will test long-distance vs. short-distance patterns)\n",
        "\n",
        "# Start with a small T and increase it later\n",
        "T = 10\n",
        "D = 1\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "def get_label(x, i1, i2, i3):\n",
        "  # x = sequence\n",
        "  if x[i1] < 0 and x[i2] < 0 and x[i3] < 0:\n",
        "    return 1\n",
        "  if x[i1] < 0 and x[i2] > 0 and x[i3] > 0:\n",
        "    return 1\n",
        "  if x[i1] > 0 and x[i2] < 0 and x[i3] > 0:\n",
        "    return 1\n",
        "  if x[i1] > 0 and x[i2] > 0 and x[i3] < 0:\n",
        "    return 1\n",
        "  return 0\n",
        "\n",
        "for t in range(5000):\n",
        "  x = np.random.randn(T)  #random noise of size T\n",
        "    #datapoint adiacenti non sono correlati, sono di classi diverse :/\n",
        "    #in pratica ho un cubo diviso in 8 quadrati distribuiti tra due classi differenti\n",
        "  X.append(x)\n",
        "  y = get_label(x, -1, -2, -3) # short distance\n",
        "#   y = get_label(x, 0, 1, 2) # long distance\n",
        "  Y.append(y)\n",
        "\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "N = len(X)\n",
        "\n",
        "# Try a linear model first - note: it is classification now!\n",
        "# Il problema è non lineare, fallisce \n",
        "i = Input(shape=(T,))\n",
        "x = Dense(1, activation='sigmoid')(i)\n",
        "model = Model(i, x)\n",
        "model.compile(\n",
        "  loss='binary_crossentropy',\n",
        "  optimizer=Adam(lr=0.01),\n",
        "  metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "# train the network\n",
        "r = model.fit(\n",
        "  X, Y,\n",
        "  epochs=100,\n",
        "  validation_split=0.5,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss\n",
        "plt.plot(r.history['loss'], label='loss')\n",
        "plt.plot(r.history['val_loss'], label='val_loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "dwc068Rtt2m5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the accuracy too - should be around 50%\n",
        "plt.plot(r.history['accuracy'], label='acc')\n",
        "plt.plot(r.history['val_accuracy'], label='val_acc')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "Xz_YG4Qtt397"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now try a simple RNN\n",
        "# Funziona bene! Perchè ciò che influenza i risultati sono gli elementi terminali, \n",
        "# ma se all'inizio cambio e metto y per la long term la loss non scenderà bene.\n",
        "inputs = np.expand_dims(X, -1)\n",
        "\n",
        "# make the RNN\n",
        "i = Input(shape=(T, D))\n",
        "\n",
        "# method 1\n",
        "# Adesso posso testare le varie soluzioni:\n",
        "# x = LSTM(5)(i)\n",
        "x = SimpleRNN(5)(i)\n",
        "# x = GRU(5)(i)\n",
        "\n",
        "# method 2\n",
        "# x = LSTM(5, return_sequences=True)(i)\n",
        "# x = GlobalMaxPool1D()(x)\n",
        "\n",
        "x = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(i, x)\n",
        "model.compile(\n",
        "  loss='binary_crossentropy',\n",
        "  # optimizer='rmsprop',\n",
        "#   optimizer='adam',\n",
        "  optimizer=Adam(lr=0.01),\n",
        "  # optimizer=SGD(lr=0.1, momentum=0.9),\n",
        "  metrics=['accuracy'],\n",
        ")\n"
      ],
      "metadata": {
        "id": "OuP84iQXt5Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In oltre posso renderlo più difficile portando T da 10 a 20, a quel punto utilizzare una LSTM è molto meglio che la simple RNN\n",
        "\n",
        "Il **method2** è pensato per quando il compito è ancora + difficile, T=30!\n",
        "Pure la LSTM fallisce perchè è troppo lunga la sequenza.\n",
        "\n",
        "La soluzione è prendere non solo l'ultimo hidden state h(T) ma prendere tutti quelli dei vari timestep. grazie a return_sequence.\n",
        "Quindi non otteniamo solo un 1xM vector ma TxM vectors.\n",
        "\n",
        "È un volume come per le CNN, possiamo applicare il max pooling e quindi alla fine riotteniamo un vettore 1xM.\n",
        "\n",
        "E così si riesce a imparare anche per sequenze molte lunghe guardando a tutti gli idden state."
      ],
      "metadata": {
        "id": "cfFLsnK9yx2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the RNN\n",
        "r = model.fit(\n",
        "  inputs, Y,\n",
        "  epochs=200,\n",
        "  validation_split=0.5,\n",
        ")\n"
      ],
      "metadata": {
        "id": "A7nJj9pLt6n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss\n",
        "plt.plot(r.history['loss'], label='loss')\n",
        "plt.plot(r.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "DCvPrhnUt9tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the accuracy too\n",
        "plt.plot(r.history['accuracy'], label='acc')\n",
        "plt.plot(r.history['val_accuracy'], label='val_acc')\n",
        "plt.legend()\n",
        "\n"
      ],
      "metadata": {
        "id": "ylLW6tcOt_N-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now change to the long distance problem\n",
        "\n",
        "# Start with a small T and increase it later\n",
        "T = 10\n",
        "D = 1\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "for t in range(5000):\n",
        "  x = np.random.randn(T)\n",
        "  X.append(x)\n",
        "  y = get_label(x, 0, 1, 2) # long distance\n",
        "  Y.append(y)\n",
        "\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "N = len(X)"
      ],
      "metadata": {
        "id": "kf57A9-cuBrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now test our Simple RNN again\n",
        "inputs = np.expand_dims(X, -1)\n",
        "\n",
        "# make the RNN\n",
        "i = Input(shape=(T, D))\n",
        "\n",
        "# method 1\n",
        "x = SimpleRNN(5)(i)\n",
        "\n",
        "x = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(i, x)\n",
        "model.compile(\n",
        "  loss='binary_crossentropy',\n",
        "  optimizer=Adam(lr=0.01),\n",
        "  metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "# train the RNN\n",
        "r = model.fit(\n",
        "  inputs, Y,\n",
        "  epochs=200,\n",
        "  validation_split=0.5,\n",
        ")"
      ],
      "metadata": {
        "id": "Lh4cU53WuDp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss\n",
        "plt.plot(r.history['loss'], label='loss')\n",
        "plt.plot(r.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "4XeotnKAuGsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the accuracy too\n",
        "plt.plot(r.history['accuracy'], label='acc')\n",
        "plt.plot(r.history['val_accuracy'], label='val_acc')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "3bHE8jR8uIB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now test our LSTM\n",
        "inputs = np.expand_dims(X, -1)\n",
        "\n",
        "# make the RNN\n",
        "i = Input(shape=(T, D))\n",
        "\n",
        "# method 1\n",
        "x = LSTM(5)(i)\n",
        "\n",
        "x = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(i, x)\n",
        "model.compile(\n",
        "  loss='binary_crossentropy',\n",
        "  optimizer=Adam(lr=0.01),\n",
        "  metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "# train the RNN\n",
        "r = model.fit(\n",
        "  inputs, Y,\n",
        "  epochs=200,\n",
        "  validation_split=0.5,\n",
        ")"
      ],
      "metadata": {
        "id": "O-eIhUVduJet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss\n",
        "plt.plot(r.history['loss'], label='loss')\n",
        "plt.plot(r.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "qEHDR622uMEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the accuracy too\n",
        "plt.plot(r.history['accuracy'], label='acc')\n",
        "plt.plot(r.history['val_accuracy'], label='val_acc')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "7bdESH30uNR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the problem harder by making T larger\n",
        "T = 20\n",
        "D = 1\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "for t in range(5000):\n",
        "  x = np.random.randn(T)\n",
        "  X.append(x)\n",
        "  y = get_label(x, 0, 1, 2) # long distance\n",
        "  Y.append(y)\n",
        "\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "N = len(X)"
      ],
      "metadata": {
        "id": "KGnZq-YvuOrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now test our Simple RNN again\n",
        "inputs = np.expand_dims(X, -1)\n",
        "\n",
        "# make the RNN\n",
        "i = Input(shape=(T, D))\n",
        "\n",
        "# method 1\n",
        "x = SimpleRNN(5)(i)\n",
        "\n",
        "x = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(i, x)\n",
        "model.compile(\n",
        "  loss='binary_crossentropy',\n",
        "  optimizer=Adam(lr=0.01),\n",
        "  metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "# train the RNN\n",
        "r = model.fit(\n",
        "  inputs, Y,\n",
        "  epochs=200,\n",
        "  validation_split=0.5,\n",
        ")\n"
      ],
      "metadata": {
        "id": "r86TIUT3uPyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss\n",
        "plt.plot(r.history['loss'], label='loss')\n",
        "plt.plot(r.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "iWF3zCHXuSOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the accuracy too\n",
        "plt.plot(r.history['accuracy'], label='acc')\n",
        "plt.plot(r.history['val_accuracy'], label='val_acc')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "48LgY-s0uTw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now test our LSTM\n",
        "inputs = np.expand_dims(X, -1)\n",
        "\n",
        "# make the RNN\n",
        "i = Input(shape=(T, D))\n",
        "\n",
        "# method 1\n",
        "x = LSTM(5)(i)\n",
        "\n",
        "x = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(i, x)\n",
        "model.compile(\n",
        "  loss='binary_crossentropy',\n",
        "  optimizer=Adam(lr=0.01),\n",
        "  metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "# train the RNN\n",
        "r = model.fit(\n",
        "  inputs, Y,\n",
        "  epochs=200,\n",
        "  validation_split=0.5,\n",
        ")"
      ],
      "metadata": {
        "id": "_Zm5v8vMuU9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss\n",
        "plt.plot(r.history['loss'], label='loss')\n",
        "plt.plot(r.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n",
        "\n"
      ],
      "metadata": {
        "id": "t7bxWAsIuXXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the accuracy too\n",
        "plt.plot(r.history['accuracy'], label='acc')\n",
        "plt.plot(r.history['val_accuracy'], label='val_acc')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "NhtB499IuYzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now test our GRU\n",
        "inputs = np.expand_dims(X, -1)\n",
        "\n",
        "# make the RNN\n",
        "i = Input(shape=(T, D))\n",
        "\n",
        "# method 1\n",
        "x = GRU(5)(i)\n",
        "\n",
        "x = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(i, x)\n",
        "model.compile(\n",
        "  loss='binary_crossentropy',\n",
        "  optimizer=Adam(lr=0.01),\n",
        "  metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "# train the RNN\n",
        "r = model.fit(\n",
        "  inputs, Y,\n",
        "  epochs=400,\n",
        "  validation_split=0.5,\n",
        ")\n"
      ],
      "metadata": {
        "id": "eJOK6D3FuZ4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss\n",
        "plt.plot(r.history['loss'], label='loss')\n",
        "plt.plot(r.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "Tshlvt3hucuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the accuracy too\n",
        "plt.plot(r.history['accuracy'], label='acc')\n",
        "plt.plot(r.history['val_accuracy'], label='val_acc')\n",
        "plt.legend()\n",
        "\n"
      ],
      "metadata": {
        "id": "siOfEQcuud3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the problem harder by making T larger\n",
        "T = 30\n",
        "D = 1\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "for t in range(5000):\n",
        "  x = np.random.randn(T)\n",
        "  X.append(x)\n",
        "  y = get_label(x, 0, 1, 2) # long distance\n",
        "  Y.append(y)\n",
        "\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "N = len(X)"
      ],
      "metadata": {
        "id": "QmAExUshufAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now test our LSTM\n",
        "inputs = np.expand_dims(X, -1)\n",
        "\n",
        "# make the RNN\n",
        "i = Input(shape=(T, D))\n",
        "\n",
        "# method 1\n",
        "x = LSTM(15)(i)\n",
        "\n",
        "x = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(i, x)\n",
        "model.compile(\n",
        "  loss='binary_crossentropy',\n",
        "  optimizer=Adam(lr=0.01),\n",
        "  metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "# train the RNN\n",
        "r = model.fit(\n",
        "  inputs, Y,\n",
        "  epochs=400,\n",
        "  validation_split=0.5,\n",
        ")"
      ],
      "metadata": {
        "id": "fS9VDouxugrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss\n",
        "plt.plot(r.history['loss'], label='loss')\n",
        "plt.plot(r.history['val_loss'], label='val_loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "kcYHKRu0ukdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the accuracy too\n",
        "plt.plot(r.history['accuracy'], label='acc')\n",
        "plt.plot(r.history['val_accuracy'], label='val_acc')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "kXwNlyoKumHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now try a LSTM with Global Max Pooling\n",
        "inputs = np.expand_dims(X, -1)\n",
        "\n",
        "# make the RNN\n",
        "i = Input(shape=(T, D))\n",
        "\n",
        "# method 2\n",
        "x = LSTM(5, return_sequences=True)(i)\n",
        "x = GlobalMaxPool1D()(x)\n",
        "\n",
        "x = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(i, x)\n",
        "model.compile(\n",
        "  loss='binary_crossentropy',\n",
        "  optimizer=Adam(lr=0.01),\n",
        "  metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "# train the RNN\n",
        "r = model.fit(\n",
        "  inputs, Y,\n",
        "  epochs=100,\n",
        "  validation_split=0.5,\n",
        ")\n"
      ],
      "metadata": {
        "id": "obbYEDxOun-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss\n",
        "plt.plot(r.history['loss'], label='loss')\n",
        "plt.plot(r.history['val_loss'], label='val_loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "bEEo6sIjuqah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss\n",
        "plt.plot(r.history['loss'], label='loss')\n",
        "plt.plot(r.history['val_loss'], label='val_loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "9iJb30kAuruH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}