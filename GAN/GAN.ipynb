{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JulIINA-x4Qc"
      },
      "outputs": [],
      "source": [
        "# Install TensorFlow\n",
        "# !pip install -q tensorflow-gpu==2.0.0-rc0\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x  # Colab only.\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "# More imports\n",
        "from tensorflow.keras.layers import Input, Dense, LeakyReLU, Dropout, \\\n",
        "  BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sys, os\n",
        "\n",
        "\n",
        "# Load in the data\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# map inputs to (-1, +1) for better training\n",
        "x_train, x_test = x_train / 255.0 * 2 - 1, x_test / 255.0 * 2 - 1\n",
        "print(\"x_train.shape:\", x_train.shape)\n",
        "\n",
        "# Flatten the data\n",
        "N, H, W = x_train.shape\n",
        "D = H * W\n",
        "x_train = x_train.reshape(-1, D)\n",
        "x_test = x_test.reshape(-1, D)\n",
        "\n",
        "# Dimensionality of the latent space (posso cambiarlo, Ã¨ un hyperparameter)\n",
        "latent_dim = 100\n",
        "\n",
        "# Get the generator model\n",
        "def build_generator(latent_dim):\n",
        "  i = Input(shape=(latent_dim,))\n",
        "  x = Dense(256, activation=LeakyReLU(alpha=0.2))(i)\n",
        "  x = BatchNormalization(momentum=0.7)(x)\n",
        "  x = Dense(512, activation=LeakyReLU(alpha=0.2))(x)\n",
        "  x = BatchNormalization(momentum=0.7)(x)\n",
        "  x = Dense(1024, activation=LeakyReLU(alpha=0.2))(x)\n",
        "  x = BatchNormalization(momentum=0.7)(x)\n",
        "  x = Dense(D, activation='tanh')(x) #Notice that because our image pixels are centered to be between -1 +1\n",
        "\n",
        "  model = Model(i, x)\n",
        "  return model\n",
        "\n",
        "# Get the discriminator model\n",
        "def build_discriminator(img_size):\n",
        "  i = Input(shape=(img_size,))\n",
        "  x = Dense(512, activation=LeakyReLU(alpha=0.2))(i)\n",
        "  x = Dense(256, activation=LeakyReLU(alpha=0.2))(x)\n",
        "  x = Dense(1, activation='sigmoid')(x)\n",
        "  model = Model(i, x)\n",
        "  return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile both models in preparation for training\n",
        "\n",
        "\n",
        "# Build and compile the discriminator\n",
        "# D = input dimensionality\n",
        "discriminator = build_discriminator(D)\n",
        "discriminator.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=Adam(0.0002, 0.5),\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "# Build and compile the combined model\n",
        "generator = build_generator(latent_dim)\n",
        "\n",
        "# we need to create a combined model where an input goes through both a generator and discriminator.\n",
        "\n",
        "# Create an input to represent noise sample from latent space\n",
        "z = Input(shape=(latent_dim,))\n",
        "\n",
        "# Pass noise through generator to get an image\n",
        "# So even though the generator is a model object it can still be used like a function to go from input to output.\n",
        "img = generator(z)\n",
        "\n",
        "# Make sure only the generator is trained\n",
        "discriminator.trainable = False\n",
        "\n",
        "#Next we pass in our image variable, which was the output of the generator, through the discriminator model.\n",
        "# it's an output prediction from our discriminator for our fake images \n",
        "\n",
        "# The true output is fake, but we label them real!\n",
        "fake_pred = discriminator(img)\n",
        "\n",
        "# Create the combined model object\n",
        "combined_model = Model(z, fake_pred)\n",
        "\n",
        "# Compile the combined model\n",
        "combined_model.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n"
      ],
      "metadata": {
        "id": "8PM0V9NayCMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the GAN\n",
        "\n",
        "\n",
        "# Config\n",
        "batch_size = 32\n",
        "epochs = 30000\n",
        "sample_period = 200 # every `sample_period` steps generate and save some data\n",
        "\n",
        "\n",
        "# Create batch labels to use when calling train_on_batch\n",
        "ones = np.ones(batch_size)\n",
        "zeros = np.zeros(batch_size)\n",
        "\n",
        "# Store the losses\n",
        "d_losses = []\n",
        "g_losses = []\n",
        "\n",
        "# Create a folder to store generated images\n",
        "if not os.path.exists('gan_images'):\n",
        "  os.makedirs('gan_images')"
      ],
      "metadata": {
        "id": "hyOHnuidyDUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PLOTTING to have some preview\n",
        "\n",
        "# A function to generate a grid of random samples from the generator\n",
        "# and save them to a file\n",
        "def sample_images(epoch):\n",
        "  rows, cols = 5, 5\n",
        "  noise = np.random.randn(rows * cols, latent_dim)\n",
        "  imgs = generator.predict(noise)\n",
        "\n",
        "  # Rescale images between 0 - 1\n",
        "  imgs = 0.5 * imgs + 0.5\n",
        "\n",
        "  fig, axs = plt.subplots(rows, cols)\n",
        "  idx = 0\n",
        "  for i in range(rows):\n",
        "    for j in range(cols):\n",
        "      axs[i,j].imshow(imgs[idx].reshape(H, W), cmap='gray')\n",
        "      axs[i,j].axis('off')\n",
        "      idx += 1\n",
        "  fig.savefig(\"gan_images/%d.png\" % epoch)\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "KG4Lu75KyETb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main training loop\n",
        "for epoch in range(epochs):\n",
        "  ###########################\n",
        "  ### Train discriminator ###\n",
        "  ###########################\n",
        "  \n",
        "  #We need both real and fake images.\n",
        "  #We can get real images by sampling from random indices from zero up to the number of \n",
        "  #samples in X_train\n",
        "  #then we can get the actual images by indexing X_train at those indices.\n",
        "\n",
        "  # Select a random batch of images\n",
        "  idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
        "  real_imgs = x_train[idx]\n",
        "  \n",
        "  # Generate fake images\n",
        "  #we create some random noise sampled from the standard normal in our latent space.\n",
        "  noise = np.random.randn(batch_size, latent_dim)\n",
        "\n",
        "  #Then we call generate.predict with this noise as input. This gives us our fake images.\n",
        "  fake_imgs = generator.predict(noise)\n",
        "  \n",
        "  #we call discriminator.train on batch\n",
        "  #first we pass in the real images and our ones vector to denote that these belong to the positive class.\n",
        "  #We get back the loss and the accuracy.\n",
        "  #Next we pass in our fake images and our zeros vector to denote that these belong to the negative class.\n",
        "  #We again get back the loss and accuracy in order to calculate the overall loss in accuracy.\n",
        "  #We're going to take the mean of these losses and accuracies.\n",
        "\n",
        "  # Train the discriminator\n",
        "  # both loss and accuracy are returned\n",
        "  d_loss_real, d_acc_real = discriminator.train_on_batch(real_imgs, ones)\n",
        "  d_loss_fake, d_acc_fake = discriminator.train_on_batch(fake_imgs, zeros)\n",
        "  d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
        "  d_acc  = 0.5 * (d_acc_real + d_acc_fake)\n",
        "  \n",
        "  \n",
        "  #######################\n",
        "  ### Train generator ###\n",
        "  #######################\n",
        "  \n",
        "  #for the generator We need to fake images only so first we generate some more noise\n",
        "\n",
        "  noise = np.random.randn(batch_size, latent_dim)\n",
        "  g_loss = combined_model.train_on_batch(noise, ones)\n",
        "\n",
        "  #we call combined.model that train on batch.\n",
        "  #The input is the noise and the target is our vector of ones.\n",
        "  #This is because we are trying to trick the discriminator into thinking that the images from the generator are real.\n",
        "  \n",
        "  #The following code has been added later probably:\n",
        "  # do it again!\n",
        "  noise = np.random.randn(batch_size, latent_dim)\n",
        "  g_loss = combined_model.train_on_batch(noise, ones)\n",
        "  \n",
        "\n",
        "  # Save the losses\n",
        "  d_losses.append(d_loss)\n",
        "  g_losses.append(g_loss)\n",
        "  \n",
        "  if epoch % 100 == 0:\n",
        "    print(f\"epoch: {epoch+1}/{epochs}, d_loss: {d_loss:.2f}, \\\n",
        "      d_acc: {d_acc:.2f}, g_loss: {g_loss:.2f}\")\n",
        "  \n",
        "  if epoch % sample_period == 0:\n",
        "    sample_images(epoch)\n",
        "\n",
        "#so if we look at the accuracy values we can see that despite the discriminator training it never reaches high accuracy.\n",
        "#This is of course because while the discriminator is improving so too is the generator and that's exactly\n",
        "#what we want obviously because we want the generator to be able to create realistic images."
      ],
      "metadata": {
        "id": "ZASBEUYDyFyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(g_losses, label='g_losses')\n",
        "plt.plot(d_losses, label='d_losses')\n",
        "plt.legend()\n",
        "\n",
        "!ls gan_images"
      ],
      "metadata": {
        "id": "KHVbPoedyH19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.io import imread\n",
        "a = imread('gan_images/0.png')\n",
        "plt.imshow(a)\n"
      ],
      "metadata": {
        "id": "rqXXHGDPyKf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = imread('gan_images/1000.png')\n",
        "plt.imshow(a)\n"
      ],
      "metadata": {
        "id": "tdJysaRvyLix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = imread('gan_images/5000.png')\n",
        "plt.imshow(a)\n"
      ],
      "metadata": {
        "id": "Ux6PK9GHyMb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = imread('gan_images/10000.png')\n",
        "plt.imshow(a)\n"
      ],
      "metadata": {
        "id": "Urea_pNoyNVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = imread('gan_images/20000.png')\n",
        "plt.imshow(a)\n"
      ],
      "metadata": {
        "id": "T6H6OnjXyOue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = imread('gan_images/29800.png')\n",
        "plt.imshow(a)\n"
      ],
      "metadata": {
        "id": "d0F6goTHyPtH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}