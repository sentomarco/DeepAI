# -*- coding: utf-8 -*-
"""dqn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_yNMHf3ViA3YsEGLJ2tNQ8iVhMDRQYEk
"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
from keras import layers
import random
from keras.layers import Flatten
import matplotlib.pyplot as plt

#!pip install import-ipynb

import envi

# Define the environment
num_actions = 3
num_rewards = 3
num_states = 4

# Define hyperparameters
learning_rate = 0.001
discount_factor = 0.99
epsilon = 1.0
epsilon_min = 0.01
epsilon_decay = 0.999
batch_size = 32
memory_size = 10000

# Define the neural network

inputs = keras.Input(shape=(num_states,), name="Input")
x = layers.Dense(30, activation="relu", name="dense_1")(inputs)
x = layers.Dense(30, activation="relu", name="dense_2")(x)
outputs = layers.Dense(num_actions, activation="softmax", name="predictions")(x)

model = keras.Model(inputs=inputs, outputs=outputs)
model.summary()

# Define the optimizer and loss function
#optimizer = tf.keras.optimizers.experimental.SGD(learning_rate=0.1)
loss_fn = 'categorical_crossentropy'

# Compile the model
model.compile(optimizer="adam",
              loss='categorical_crossentropy',
              metrics=['accuracy'])

#model.compile(loss=loss_fn, optimizer=optimizer, metrics=['accuracy'])

# Define the experience replay buffer
experience = {'s': [], 'a': [], 'r': [], 's2': [], 'done': []}
rewards_history = []
rew_index = 0

# Define the function to select an action
def select_action(state):
	global epsilon, model
	if np.random.rand() < epsilon:
		return random.randrange(0,num_actions)
	else:
		#state=np.ndarray((1,4), buffer=np.array(state))
		state=np.reshape(state, (1,4))
		#print('else', state ,state.shape)
		q_values = model.predict(state)
		#print(q_values)
		print("next actions: ", q_values[0]) # q_values[0] becouse is in this format: [[1,2,3]]
		return np.argmax(q_values[0])


# Define the function to train the model
def train_model():

	global epsilon, model
	if len(experience['s']) < batch_size:
		pass
	else:
		batch = np.random.choice(len(experience['s']), size=batch_size, replace=False)
		#batch = random.sample(memory, batch_size)
		states = [experience['s'][i] for i in batch]
		actions = [experience['a'][i] for i in batch]
		rewards = [experience['r'][i] for i in batch]
		next_states = [experience['s2'][i] for i in batch]
		done_flags = [experience['done'][i] for i in batch]
		
		#print(states)
		#q_values = model.predict(states)
		
		next_q_values = model.predict(next_states)
		max_next_q_values = np.max(next_q_values, axis=1)
		
		targets=np.empty([32, 3])
		i=0
		#targets = rewards + (1 - done_flags) * discount_factor * max_next_q_values
		for r, next_q, done in zip(rewards, next_q_values, done_flags):
			
			if not done:  targets[i]=r + discount_factor*next_q
			else:         targets[i]=r
			i+=1
		
		#print(states)
		states=np.array(states)
		#print(states.shape)
		model.train_on_batch(states, targets)
		
		if epsilon > epsilon_min:
			epsilon *= epsilon_decay
			#print(epsilon)


def history():
	global rew_index
	#print(memory[0:rew_index][2])
	tmp=0
	if rew_index > 1000:
		for i in range(rew_index-1000,rew_index):	tmp+=experience['r'][i]/(1000+1)
	else:
		for i in range(0,rew_index):	tmp+=experience['r'][i]/(rew_index+1)

	rewards_history.append(tmp)
	rew_index+=1


def update_experience(state, action, reward, next_state, done):

	if len(experience['s']) >= memory_size:
		experience['s'].pop(0)
		experience['a'].pop(0)
		experience['r'].pop(0)
		experience['s2'].pop(0)
		experience['done'].pop(0)
		
	experience['s'].append(state)
	experience['a'].append(action)
	experience['r'].append(reward)
	experience['s2'].append(next_state)
	experience['done'].append(done)
	



# Define the function to interact with the environment
def interact(env, num_episodes):

	for episode in range(num_episodes):
		state = env.reset()
		done = False
		total_reward = 0
		iters=0
		while not done and iters < 2000:
			
			action = select_action(state)
			#print(action)
			
			next_state, reward, done = env.update_state(action)
			total_reward += reward
			
			update_experience(state, action, reward, next_state, done)

			state = next_state
			train_model()
			#print(state)
			#print("Iteration: ", iters)
			iters+=1
		print(f"\n\n\nEpisode {episode}: total reward = {total_reward}\n\n\n")
		print("*************************************************************************************************************************************")
		history()



def score():
        return sum(rewards_history)/(len(rewards_history)+1.)


env=envi.Enviroment(num_actions, num_rewards)
interact(env,500)

plt.plot(rewards_history)
plt.show()
