{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9WJPhhijQfR"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Install TensorFlow\n",
        "# !pip install -q tensorflow-gpu==2.0.0-beta1\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x  # Colab only.\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Unfortunately this URL doesn't work directly with pd.read_csv\n",
        "!wget -nc https://lazyprogrammer.me/course_files/spam.csv\n",
        "\n",
        "df = pd.read_csv('spam.csv', encoding='ISO-8859-1')\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# drop unnecessary columns\n",
        "df = df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
        "\n",
        "df.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "o_cEaZZwjxhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rename columns to something better\n",
        "df.columns = ['labels', 'data']\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Gcc6l6KKjzgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create binary labels\n",
        "df['b_labels'] = df['labels'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "Y = df['b_labels'].values #Binary rapresentation of the values, if the label is ham   \n",
        "                          #now has associated 0 and if it's spam has 1\n",
        "\n",
        "# split up the data\n",
        "df_train, df_test, Ytrain, Ytest = train_test_split(df['data'], Y, test_size=0.33)\n",
        "\n",
        "# Convert sentences to sequences\n",
        "MAX_VOCAB_SIZE = 20000\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts(df_train)\n",
        "sequences_train = tokenizer.texts_to_sequences(df_train)\n",
        "sequences_test = tokenizer.texts_to_sequences(df_test)\n",
        "\n",
        "# get word -> integer mapping\n",
        "word2idx = tokenizer.word_index\n",
        "V = len(word2idx)\n",
        "print('Found %s unique tokens.' % V)\n",
        "\n",
        "# pad sequences so that we get a N x T matrix\n",
        "data_train = pad_sequences(sequences_train)\n",
        "print('Shape of data train tensor:', data_train.shape)\n",
        "\n",
        "# get sequence length\n",
        "T = data_train.shape[1]\n",
        "\n",
        "data_test = pad_sequences(sequences_test, maxlen=T)\n",
        "print('Shape of data test tensor:', data_test.shape)\n"
      ],
      "metadata": {
        "id": "edV0gSuIjzcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "\n",
        "# We get to choose embedding dimensionality\n",
        "D = 20\n",
        "\n",
        "# Note: we actually want to the size of the embedding to (V + 1) x D,\n",
        "# because the first index starts from 1 and not 0.\n",
        "# Thus, if the final index of the embedding matrix is V,\n",
        "# then it actually must have size V + 1.\n",
        "\n",
        "i = Input(shape=(T,))\n",
        "x = Embedding(V + 1, D)(i)\n",
        "x = Conv1D(32, 3, activation='relu')(x)\n",
        "x = MaxPooling1D(3)(x)\n",
        "x = Conv1D(64, 3, activation='relu')(x)\n",
        "x = MaxPooling1D(3)(x)\n",
        "x = Conv1D(128, 3, activation='relu')(x)\n",
        "x = GlobalMaxPooling1D()(x)\n",
        "x = Dense(1, activation='sigmoid')(x) #sigmoid since it is binary classification\n",
        "\n",
        "model = Model(i, x)"
      ],
      "metadata": {
        "id": "IgSBMuASj19w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and fit\n",
        "model.compile(\n",
        "  loss='binary_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")\n",
        "\n",
        "\n",
        "print('Training model...')\n",
        "r = model.fit(\n",
        "  data_train,\n",
        "  Ytrain,\n",
        "  epochs=5,\n",
        "  validation_data=(data_test, Ytest)\n",
        ")"
      ],
      "metadata": {
        "id": "8H_SGwzYj5WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot loss per iteration\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(r.history['loss'], label='loss')\n",
        "plt.plot(r.history['val_loss'], label='val_loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "-m-X78hlj7nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot accuracy per iteration\n",
        "plt.plot(r.history['accuracy'], label='acc')\n",
        "plt.plot(r.history['val_accuracy'], label='val_acc')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "rjW6DKeNj9dQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}